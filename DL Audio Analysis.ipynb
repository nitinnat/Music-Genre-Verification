{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa \n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from scipy import misc\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "SAMPLE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(music_dir):\n",
    "    \n",
    "    folders = [os.path.join(music_dir,folder) for folder in list(os.walk(music_dir))[0][1]]\n",
    "    print(folders[0])\n",
    "    filenames = [[os.path.join(folder,f) for f in list(os.walk(folder))[0][2]] for folder in folders]\n",
    "    filenames = [item for sublist in filenames for item in sublist]\n",
    "    files = [item for item in filenames]\n",
    "    track_ids = [str(int(filename.split('\\\\')[-1].split('.')[0])) for filename in filenames]\n",
    "    print(files[0:10])\n",
    "    print(track_ids[0:10])\n",
    "    print(\"Number of files: \" + str(len(files)))\n",
    "    return files, track_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOT NEEDED for current analysis.\n",
    "Load audio files, aggregate them over 100 sample intervals. Find mean, max and min for the features.\n",
    "\"\"\"\n",
    "def process_audio(files,track_ids,window = 100,sample = SAMPLE):\n",
    "    clip_range = int(19800*SAMPLE/window)  ##To clip the feature vectors for equal length.\n",
    "    train_data = []\n",
    "    num_files = len(files[0:sample])\n",
    "    for i in tqdm(range(num_files)):\n",
    "        audio_vec = []\n",
    "        audio, _ = librosa.load(files[i])\n",
    "        audio = np.reshape(audio,(-1))\n",
    "        for j in range(1,int(audio.shape[0]/window)):\n",
    "            mean_val = np.mean(audio[j*window: min((j+1)*window,audio.shape[0])])\n",
    "            max_val = np.max(audio[j*window: min((j+1)*window,audio.shape[0])])\n",
    "            min_val = np.min(audio[j*window: min((j+1)*window,audio.shape[0])])\n",
    "            #Append to audio vector for this audio file.\n",
    "            audio_vec += [mean_val,max_val,min_val]\n",
    "        audio_vec = np.array(audio_vec[:clip_range])\n",
    "        train_data.append(np.array(audio_vec))\n",
    "    train_data = np.array(train_data)\n",
    "    print(\"Original audio shape: \" + str(audio.shape))\n",
    "    print(\"Condensed audio shape: \" + str(train_data.shape))\n",
    "    train_ids = track_ids[0:num_files]\n",
    "    return train_data, train_ids\n",
    "#train_data, train_ids = process_audio(files,track_ids, 200,200)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now aggregate results found in tracks.csv and get the labels.\n",
    "\n",
    "def load_tracks_file(filepath):\n",
    "    tracks = pd.read_csv(filepath)\n",
    "    ##Set new columns for the dataframe and remove the multi-index.\n",
    "    new_cols = tracks.iloc[0]\n",
    "    tracks = tracks.iloc[1:]\n",
    "    new_cols[0] = \"track_id\"\n",
    "    tracks.columns = new_cols\n",
    "    labels = tracks[\"genre_top\"]\n",
    "    \n",
    "    #Track id column should be string type\n",
    "    tracks.track_id = tracks.track_id.astype(int).astype(str)\n",
    "    #tracks = tracks.set_index(\"track_id\")\n",
    "    return tracks,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the melspectrogram from the audio files.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import librosa as lb\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "Fs         = 12000\n",
    "N_FFT      = 512\n",
    "N_MELS     = 96\n",
    "N_OVERLAP  = 256\n",
    "DURA       = 29.12\n",
    "\n",
    "def log_scale_melspectrogram(path, plot=False):\n",
    "    signal, sr = lb.load(path, sr=Fs)\n",
    "\n",
    "    n_sample = signal.shape[0]\n",
    "    n_sample_fit = int(DURA*Fs)\n",
    "    \n",
    "    if n_sample < n_sample_fit:\n",
    "        signal = np.hstack((signal, np.zeros((int(DURA*Fs) - n_sample,))))\n",
    "    elif n_sample > n_sample_fit:\n",
    "        signal = signal[round((n_sample-n_sample_fit)/2):round((n_sample+n_sample_fit)/2)]\n",
    "    \n",
    "    melspect = lb.amplitude_to_db(lb.feature.melspectrogram(y=signal, sr=Fs, hop_length=N_OVERLAP, n_fft=N_FFT, n_mels=N_MELS)**2, ref=1.0)\n",
    "\n",
    "    if plot:\n",
    "        melspect = melspect[np.newaxis, :]\n",
    "        plt.imshow(melspect.reshape((melspect.shape[1],melspect.shape[2])))\n",
    "        print(melspect.shape)\n",
    "\n",
    "    return melspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_mel_features(filenames):\n",
    "    arr = np.zeros([len(filenames), 96, 1366])\n",
    "    track_ids = [str(int(filename.split('\\\\')[-1].split('.')[0])) for filename in filenames]\n",
    "    for i in tqdm(range(len(filenames))):\n",
    "        mel = log_scale_melspectrogram(filenames[i], plot = False)\n",
    "        arr[i,:,:] = mel\n",
    "    return arr, track_ids\n",
    "\n",
    "\"\"\"\n",
    "Now, to determine the labels for the audio files that we have.\n",
    "\"\"\"\n",
    "\n",
    "def find_new_labels(train_ids, tracks_df):\n",
    "    labels_final = []\n",
    "    failed_indices = []\n",
    "    for i,tr in enumerate(train_ids):\n",
    "        try:\n",
    "            labels_final.append(tracks_df[tracks_df.track_id == tr][\"genre_top\"].values[0])\n",
    "        except:\n",
    "            #Store the index and track_id of such failed rows - and remove these in the audio features\n",
    "            failed_indices.append((i,tr))\n",
    "    #One hot encode\n",
    "    label_dic = {label: i for (i,label) in enumerate(np.unique(labels_final))} #To store numerical values for each label\n",
    "    label_nums = np.array([label_dic[l] for l in labels_final]).reshape((-1,1))\n",
    "    print(label_nums.shape)\n",
    "    one_hot_labels = OneHotEncoder().fit_transform(label_nums).todense()\n",
    "    return one_hot_labels, label_dic,failed_indices\n",
    "\n",
    "\n",
    "def make_splits(melspectrogram,one_hot_labels,test_size = 0.33):\n",
    "    return train_test_split(melspectrogram, one_hot_labels, test_size = test_size, random_state = 42)\n",
    "\n",
    "\n",
    "  \n",
    "#X_train, X_test, y_train, y_test = make_splits(melspectrogram,one_hot_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing data...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./melspectrogram1.pkl\"):\n",
    "    #Load audio\n",
    "    music_dir = \"./music_samples/\"\n",
    "    track_details_path = \"./tracks_small.csv\"\n",
    "    files, track_ids = load_audio(music_dir)\n",
    "\n",
    "    #Load csv file\n",
    "    filepath = \"./tracks_small.csv\"\n",
    "    tracks_df, labels = load_tracks_file(filepath)\n",
    "\n",
    "    #Process melspectrograms from the audio files\n",
    "    melspectrogram, track_ids = obtain_mel_features(files[0:4000])\n",
    "    one_hot_labels, label_dic = find_new_labels(track_ids,tracks_df)\n",
    "    pickle.dump({\"melspectrogram\": melspectrogram, \"one_hot_labels\":one_hot_labels, \"label_dic\":label_dic},\n",
    "                open(\"audio_data_labels.pkl\",'wb'))\n",
    "    pickle.dump(melspectrogram[0:1000],open(\"./melspectrogram1.pkl\",'wb'),protocol =2)\n",
    "    pickle.dump(melspectrogram[1001:2000],open(\"./melspectrogram2.pkl\",'wb'),protocol =2)\n",
    "    pickle.dump(melspectrogram[2001:3000],open(\"./melspectrogram3.pkl\",'wb'),protocol =2)\n",
    "    pickle.dump(melspectrogram[3001:4000],open(\"./melspectrogram4.pkl\",'wb'),protocol =2)\n",
    "    pickle.dump(track_ids,open(\"./track_ids.pkl\",'wb'),protocol =2)\n",
    "else:\n",
    "    print(\"Loading existing data...\")\n",
    "    melspectrogram = pickle.load(open('./melspectrogram1.pkl','rb'))\n",
    "    track_ids = pickle.load(open('./track_ids.pkl','rb'))[0:len(melspectrogram)]\n",
    "tracks_df, labels = load_tracks_file(\"./tracks_small.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(994, 1)\n",
      "Original melspectrogram shape: 1000\n",
      "Number of failed indicies: 6\n",
      "New melspectrogram shape: 994\n",
      "New track_ids shape: 994\n",
      "6 indices were removed\n"
     ]
    }
   ],
   "source": [
    "def remove_failed_indices(melspectrogram, track_ids, failed_inds):\n",
    "    print(\"Original melspectrogram shape: \" + str(len(melspectrogram)))\n",
    "    print(\"Number of failed indicies: \" + str(len(failed_inds)))\n",
    "    #First perform assertions to check if all the correct ids were obtained    \n",
    "    for i in range(len(failed_inds)):\n",
    "        assert track_ids[failed_inds[i][0]] ==  failed_inds[i][1], \"Failed at index \" + str(i)\n",
    "    #Remove these from the audio features\n",
    "    melspectrogram_new = [m for i,m in enumerate(melspectrogram) if not i in list(zip(*failed_inds))[0]]\n",
    "    track_ids_new = [t for i,t in enumerate(track_ids) if not i in list(zip(*failed_inds))[0]]\n",
    "    print(\"New melspectrogram shape: \" + str(len(melspectrogram_new)))\n",
    "    print(\"New track_ids shape: \" + str(len(track_ids_new)))\n",
    "    print(\"%d indices were removed\" %len(failed_inds))\n",
    "    assert len(melspectrogram_new) == len(track_ids_new) == len(melspectrogram) - len(failed_inds) \n",
    "    return melspectrogram_new, track_ids_new\n",
    "\n",
    "\n",
    "##Get the labels for these audio features and remove any indices that fail due to some index reason.\n",
    "one_hot_labels, label_dic,failed_inds = find_new_labels(track_ids,tracks_df)\n",
    "mel_new, trs_new = remove_failed_indices(melspectrogram,track_ids,failed_inds)\n",
    "mel_new = np.array(mel_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indicators: 0 One Hot[[1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X1,X2,indicators = form_verification_dataset(mel_new,one_hot_labels,sample=50)\n",
    "pickle.dump(X1, open(\"X1_data.pkl\",'wb'),protocol =2)\n",
    "pickle.dump(X2, open(\"X2_data.pkl\",'wb'),protocol =2)\n",
    "pickle.dump(indicators, open(\"indicator_data.pkl\",'wb'),protocol =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mel_new,open(\"./melspectrogram1.pkl\",'wb'),protocol =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(x, n_out, phase_train, scope='bn'):\n",
    "    with tf.variable_scope(scope):\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        mean, var = tf.cond(phase_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converts original dataset into the verification format. i.e. Input spectrograms X1, X2 and an output of 0 if they're of \n",
    "dissimilar genre and 1 if they're of the same genre.\n",
    "Input: data , labels, sample (How many input samples do you want to transform?)\n",
    "Output: Three lists X1, X2, indicators\n",
    "\"\"\"\n",
    "def form_verification_dataset(data,labels,sample):\n",
    "    ##Insert assert statements here for the correct input data sizes\n",
    "    assert len(data) == len(labels)\n",
    "    #Select samples\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:sample]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "    \n",
    "    #Pair each sample with every other sample from the dataset. O(sample^2)\n",
    "    num_mels,mel_vals = data.shape[1], data.shape[2]\n",
    "    X1 = X2 = np.zeros((sample*(sample-1)//2,num_mels,mel_vals))\n",
    "    indicators = []\n",
    "    count = 0\n",
    "    for i in range(0,len(data_shuffle)):\n",
    "        for j in range(i+1,len(data_shuffle)):\n",
    "            X1[count,:,:] = data_shuffle[i]\n",
    "            X1[count,:,:] = data_shuffle[j]\n",
    "            if np.equal(labels_shuffle[i], labels_shuffle[j]).all():\n",
    "                indicators.append(0)\n",
    "            else:\n",
    "                indicators.append(1)\n",
    "            count += 1\n",
    "            \n",
    "    one_hot = OneHotEncoder().fit_transform(np.array(indicators).reshape(-1,1)).todense()\n",
    "    print(\"Indicators: \" + str(indicators[0]) + \" One Hot\" + str(one_hot[0]))\n",
    "    return X1, X2, one_hot.reshape(-1,2)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def init_biases(shape):\n",
    "    return tf.Variable(tf.zeros(shape))\n",
    "\n",
    "weights = {\n",
    "        'wconv1':init_weights([3, 3, 1, 32]),\n",
    "        'wconv2':init_weights([3, 3, 32, 128]),\n",
    "        'wconv3':init_weights([3, 3, 128, 128]),\n",
    "        'wconv4':init_weights([3, 3, 128, 192]),\n",
    "        'wconv5':init_weights([3, 3, 192, 256]),\n",
    "        'bconv1':init_biases([32]),\n",
    "        'bconv2':init_biases([128]),\n",
    "        'bconv3':init_biases([128]),\n",
    "        'bconv4':init_biases([192]),\n",
    "        'bconv5':init_biases([256]),\n",
    "        'woutput':init_weights([256, 128]),\n",
    "        'boutput':init_biases([128]),\n",
    "        'woutput2':init_weights([256, 128]),\n",
    "        'boutput2':init_biases([128]),\n",
    "        'wfinal':init_weights([256, 2]),\n",
    "        'bfinal':init_biases([2]),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0.2\n",
    "\n",
    "def dual_cnn(X_first,X_second, weights, phase_train,keep_prob):\n",
    "    \n",
    "    #assert X_first.shape == X_second.shape, str(X_first.shape) + \" does not match with \" + str(X_second.shape)\n",
    "    #assert indicators.shape[0] == X_first.shape[0], \"Indicator array is not of length \" + indicators.shape[0]\n",
    "    \n",
    "    num_samples, n_mels, mel_vals = X1_train.shape[0],X1_train.shape[1],X1_train.shape[2]\n",
    "    x_first = X_first\n",
    "    x_second = X_second\n",
    "    x_first = batch_norm(x_first, mel_vals, phase_train)\n",
    "    x_second = batch_norm(x_first, mel_vals, phase_train)\n",
    "    \n",
    "    x_first = tf.reshape(x_first,[-1,n_mels,mel_vals,1])\n",
    "    x_second = tf.reshape(x_second,[-1,n_mels,mel_vals,1])\n",
    "    \n",
    "    conv2_1_first = tf.add(tf.nn.conv2d(x_first, weights['wconv1'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv1'])\n",
    "    conv2_1_second = tf.add(tf.nn.conv2d(x_second, weights['wconv1'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv1'])\n",
    "    \n",
    "    conv2_1_first = tf.nn.relu(batch_norm(conv2_1_first, 32, phase_train))\n",
    "    conv2_1_second = tf.nn.relu(batch_norm(conv2_1_second, 32, phase_train))\n",
    "    \n",
    "    mpool_1_first = tf.nn.max_pool(conv2_1_first, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    mpool_1_second = tf.nn.max_pool(conv2_1_second, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    \n",
    "    dropout_1_first = tf.nn.dropout(mpool_1_first, keep_prob)\n",
    "    dropout_1_second = tf.nn.dropout(mpool_1_second, keep_prob)\n",
    "\n",
    "    conv2_2_first = tf.add(tf.nn.conv2d(dropout_1_first, weights['wconv2'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv2'])\n",
    "    conv2_2_second = tf.add(tf.nn.conv2d(dropout_1_second, weights['wconv2'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv2'])\n",
    "    \n",
    "    conv2_2_first = tf.nn.relu(batch_norm(conv2_2_first, 128, phase_train))\n",
    "    conv2_2_second = tf.nn.relu(batch_norm(conv2_2_first, 128, phase_train))\n",
    "    \n",
    "    mpool_2_first = tf.nn.max_pool(conv2_2_first, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    mpool_2_second = tf.nn.max_pool(conv2_2_first, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    \n",
    "    \n",
    "    dropout_2_first = tf.nn.dropout(mpool_2_first, keep_prob)\n",
    "    dropout_2_second = tf.nn.dropout(mpool_2_second, keep_prob)\n",
    "\n",
    "    conv2_3_first = tf.add(tf.nn.conv2d(dropout_2_first, weights['wconv3'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv3'])\n",
    "    conv2_3_second = tf.add(tf.nn.conv2d(dropout_2_second, weights['wconv3'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv3'])\n",
    "    \n",
    "\n",
    "    conv2_3_first = tf.nn.relu(batch_norm(conv2_3_first, 128, phase_train))\n",
    "    conv2_3_second = tf.nn.relu(batch_norm(conv2_3_second, 128, phase_train))\n",
    "    \n",
    "    mpool_3_first = tf.nn.max_pool(conv2_3_first, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    mpool_3_second = tf.nn.max_pool(conv2_3_second, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    \n",
    "    dropout_3_first = tf.nn.dropout(mpool_3_first, keep_prob)\n",
    "    dropout_3_second = tf.nn.dropout(mpool_3_second, keep_prob)\n",
    "\n",
    "    conv2_4_first = tf.add(tf.nn.conv2d(dropout_3_first, weights['wconv4'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv4'])\n",
    "    conv2_4_second = tf.add(tf.nn.conv2d(dropout_3_second, weights['wconv4'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv4'])\n",
    "    \n",
    "    conv2_4_first = tf.nn.relu(batch_norm(conv2_4_first, 192, phase_train))\n",
    "    conv2_4_second = tf.nn.relu(batch_norm(conv2_4_second, 192, phase_train))\n",
    "    \n",
    "    mpool_4_first = tf.nn.max_pool(conv2_4_first, ksize=[1, 3, 5, 1], strides=[1, 3, 5, 1], padding='VALID')\n",
    "    mpool_4_second = tf.nn.max_pool(conv2_4_second, ksize=[1, 3, 5, 1], strides=[1, 3, 5, 1], padding='VALID')\n",
    "    \n",
    "    dropout_4_first = tf.nn.dropout(mpool_4_first, keep_prob)\n",
    "    dropout_4_second = tf.nn.dropout(mpool_4_second, keep_prob)\n",
    "\n",
    "    conv2_5_first = tf.add(tf.nn.conv2d(dropout_4_first, weights['wconv5'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv5'])\n",
    "    conv2_5_second = tf.add(tf.nn.conv2d(dropout_4_second, weights['wconv5'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv5'])\n",
    "    \n",
    "    \n",
    "    conv2_5_first = tf.nn.relu(batch_norm(conv2_5_first, 256, phase_train))\n",
    "    conv2_5_second = tf.nn.relu(batch_norm(conv2_5_second, 256, phase_train))\n",
    "    \n",
    "    mpool_5_first = tf.nn.max_pool(conv2_5_first, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='VALID')\n",
    "    mpool_5_second = tf.nn.max_pool(conv2_5_second, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='VALID')\n",
    "    \n",
    "    dropout_5_first = tf.nn.dropout(mpool_5_first, keep_prob)\n",
    "    dropout_5_second = tf.nn.dropout(mpool_5_second, keep_prob)\n",
    "    \n",
    "    flat_first = tf.reshape(dropout_5_first, [-1, weights['woutput'].get_shape().as_list()[0]])\n",
    "    flat_second = tf.reshape(dropout_5_second, [-1, weights['woutput'].get_shape().as_list()[0]])\n",
    "    \n",
    "    print(flat_first.get_shape())\n",
    "    print(flat_second.get_shape())\n",
    "    flat_first = tf.add(tf.matmul(flat_first, weights['woutput2']), weights['boutput2'])\n",
    "    flat_second = tf.add(tf.matmul(flat_second, weights['woutput2']), weights['boutput2'])\n",
    "    \n",
    "    flat_first = tf.nn.relu(flat_first)\n",
    "    flat_second = tf.nn.relu(flat_second)\n",
    "    # Apply Dropout\n",
    "    flat_first = tf.nn.dropout(flat_first, keep_prob)\n",
    "    flat_second = tf.nn.dropout(flat_second, keep_prob)\n",
    "    \n",
    "    print(flat_first.get_shape())\n",
    "    print(flat_second.get_shape())\n",
    "    \n",
    "    final_layer = tf.concat([flat_first, flat_second],1)\n",
    "    #final_layer = tf.add(tf.matmul(final_layer, weights['wfinal']), weights['bfinal'])\n",
    "    print(final_layer.get_shape())\n",
    "    \"\"\"\n",
    "    ##Add fully connected layers here\n",
    "    dense_first = tf.layers.dense(inputs=flat_first, units=128, activation=tf.nn.relu)\n",
    "    dense_second = tf.layers.dense(inputs=flat_second, units=128, activation=tf.nn.relu)\n",
    "    print(dense_first.shape)\n",
    "    print(dense_second.shape)\n",
    "    final_layer = tf.reshape(tf.concat([dense_first, dense_second], 0),(1,-1))\n",
    "    print(final_layer.shape)\n",
    "    \"\"\"\n",
    "    p_y_X = tf.nn.sigmoid(tf.add(tf.matmul(final_layer,weights['wfinal']),weights['bfinal']))\n",
    "    print(p_y_X.get_shape())\n",
    "    \n",
    "    return p_y_X\n",
    "    \"\"\"\n",
    "    #Contrastive loss\n",
    "    d = tf.reduce_sum(tf.square(dense_first - dense_second), 1)\n",
    "    d_sqrt = tf.sqrt(d)\n",
    "    loss = label * tf.square(tf.maximum(0., margin - d_sqrt)) + (1 - label) * d\n",
    "    loss = 0.5 * tf.reduce_mean(loss)\n",
    "    return loss\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * (100 - 1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mel_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-57e54b266dca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Load the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindicators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mform_verification_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmel_new\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mone_hot_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindicators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmel_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mel_new' is not defined"
     ]
    }
   ],
   "source": [
    "#Load the dataset\n",
    "X1,X2,indicators = form_verification_dataset(mel_new,one_hot_labels,sample=10)\n",
    "print(indicators.shape)\n",
    "num_samples, n_mels, mel_vals = X1.shape[0],X1.shape[1],X1.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1_train: (820, 96, 1366, 1)\n",
      "X2_train: (820, 96, 1366, 1)\n",
      "X1_test: (405, 96, 1366, 1)\n",
      "X2_test: (405, 96, 1366, 1)\n",
      "y_train: (820, 2)\n",
      "y_test: (405, 2)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as sm\n",
    "batch_size    = 1\n",
    "learning_rate = 0.003\n",
    "n_epoch       = 50\n",
    "n_samples     = 1000#len(melspectrogram)                              # change to 1000 for entire dataset\n",
    "cv_split      = 0.8                             \n",
    "train_size    = int(n_samples * cv_split)                               \n",
    "test_size     = n_samples - train_size\n",
    "n_mels = 96\n",
    "mel_vals = 1366\n",
    "\"\"\"\n",
    "Load X1, X2, indicators and track IDS.\n",
    "\"\"\"\n",
    "\n",
    "track_ids = pickle.load(open('track_ids.pkl','rb'))\n",
    "X1 = pickle.load(open(\"X1_data.pkl\",'rb'))\n",
    "X2 = pickle.load(open(\"X2_data.pkl\",'rb'))\n",
    "indicators = pickle.load(open(\"indicator_data.pkl\",'rb'))\n",
    "\n",
    "\n",
    "#Split into training and testing\n",
    "def make_splits_verif(X1,X2,indicators):\n",
    "    inds = list(range(0,len(X1)))\n",
    "    train_inds,test_inds,_,_ = train_test_split(inds,inds,random_state = 42, test_size = 0.33)\n",
    "    X1_train = np.asarray([X1[idx] for idx in train_inds]).reshape(-1, n_mels,mel_vals,1)\n",
    "    X1_test = np.asarray([X1[idx] for idx in test_inds]).reshape(-1, n_mels,mel_vals,1)\n",
    "    \n",
    "    X2_train = np.asarray([X2[idx] for idx in train_inds]).reshape(-1, n_mels,mel_vals,1)\n",
    "    X2_test = np.asarray([X1[idx] for idx in test_inds]).reshape(-1, n_mels,mel_vals,1)\n",
    "    \n",
    "    y_train = np.asarray([indicators[idx] for idx in train_inds]).reshape(-1,2) \n",
    "    y_test = np.asarray([indicators[idx] for idx in test_inds]).reshape(-1,2)\n",
    "    print(\"X1_train: \" + str(X1_train.shape))\n",
    "    print(\"X2_train: \" + str(X2_train.shape))\n",
    "    print(\"X1_test: \" + str(X1_test.shape))\n",
    "    print(\"X2_test: \" + str(X2_test.shape))\n",
    "    print(\"y_train: \" + str(y_train.shape))\n",
    "    print(\"y_test: \" + str(y_test.shape))\n",
    "    return X1_train, X2_train, y_train, X1_test, X2_test, y_test\n",
    "\n",
    "def next_batch_verif(num, X1, X2, indicators):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    num_samples, n_mels, mel_vals = X1.shape[0],X1.shape[1],X1.shape[2]\n",
    "    idx = np.arange(0 , len(X1))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    X1_shuffle = [X1[i] for i in idx]\n",
    "    X2_shuffle = [X2[i] for i in idx]\n",
    "    indicators_shuffle = [indicators[i] for i in idx]\n",
    "    return (np.asarray(X1_shuffle).reshape((num,n_mels,mel_vals,1)), \n",
    "            np.asarray(X1_shuffle).reshape((num,n_mels,mel_vals,1)), \n",
    "            np.asarray(indicators_shuffle).reshape(-1,2))\n",
    "\n",
    "\n",
    "#Make data splits\n",
    "X1_train, X2_train, y_train, X1_test, X2_test, y_test = make_splits_verif(X1,X2,indicators)\n",
    "\n",
    "#Clear memory\n",
    "del X1, X2, indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 256)\n",
      "(?, 256)\n",
      "(?, 128)\n",
      "(?, 128)\n",
      "(?, 256)\n",
      "(?, 2)\n",
      "(?, 2)\n",
      "(?, 2) (?, 2)\n",
      "WARNING:tensorflow:From <ipython-input-19-932d1cad3320>:14: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-932d1cad3320>:14: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_first = tf.placeholder(\"float\", [None, X1_train.shape[1], X1_train.shape[2], 1],name=\"First_input_vector\")\n",
    "X_second = tf.placeholder(\"float\", [None, X1_train.shape[1], X1_train.shape[2], 1],name=\"Second_input_vector\")\n",
    "\n",
    "y = tf.placeholder(\"float\", [None, 2],name=\"Truth_labels\")\n",
    "lrate = tf.placeholder(\"float\",name=\"Learning_rate\")\n",
    "keep_prob = tf.placeholder(\"float\",name=\"Dropout\")\n",
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "y_ = dual_cnn(X_first, X_second, weights, phase_train,keep_prob)\n",
    "print(y_.shape)\n",
    "predict_op = y_\n",
    "print(y.shape,y_.shape)\n",
    "# Train and Evaluate Model\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = y_))\n",
    "train_op = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 96, 1366, 1)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[20,96,1366,1366] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: bn/batchnorm/mul_1 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_First_input_vector_2_0_1/_5, bn/batchnorm/mul)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: Mean/_147 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3118_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'bn/batchnorm/mul_1', defined at:\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-932d1cad3320>\", line 9, in <module>\n    y_ = dual_cnn(X_first, X_second, weights, phase_train,keep_prob)\n  File \"<ipython-input-18-80be156476eb>\", line 11, in dual_cnn\n    x_first = batch_norm(x_first, mel_vals, phase_train)\n  File \"<ipython-input-2-965bee7e3c05>\", line 16, in batch_norm\n    normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 833, in batch_normalization\n    return x * inv + (\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 934, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1161, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 3091, in _mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[20,96,1366,1366] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: bn/batchnorm/mul_1 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_First_input_vector_2_0_1/_5, bn/batchnorm/mul)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: Mean/_147 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3118_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20,96,1366,1366] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: bn/batchnorm/mul_1 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_First_input_vector_2_0_1/_5, bn/batchnorm/mul)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: Mean/_147 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3118_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-2768f8854a0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                             \u001b[0mphase_train\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                             keep_prob: 0.5}\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_input_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20,96,1366,1366] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: bn/batchnorm/mul_1 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_First_input_vector_2_0_1/_5, bn/batchnorm/mul)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: Mean/_147 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3118_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'bn/batchnorm/mul_1', defined at:\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-932d1cad3320>\", line 9, in <module>\n    y_ = dual_cnn(X_first, X_second, weights, phase_train,keep_prob)\n  File \"<ipython-input-18-80be156476eb>\", line 11, in dual_cnn\n    x_first = batch_norm(x_first, mel_vals, phase_train)\n  File \"<ipython-input-2-965bee7e3c05>\", line 16, in batch_norm\n    normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 833, in batch_normalization\n    return x * inv + (\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 934, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1161, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 3091, in _mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Nitin\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[20,96,1366,1366] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: bn/batchnorm/mul_1 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_First_input_vector_2_0_1/_5, bn/batchnorm/mul)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: Mean/_147 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3118_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(config=config) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for i in range(n_epoch):\n",
    "        X1_train_batch,X2_train_batch,y_train_batch = next_batch_verif(batch_size,X1_train,X2_train,y_train)\n",
    "        print(X1_train_batch.shape)\n",
    "        train_input_dict = {X_first: X1_train_batch,\n",
    "                            X_second: X2_train_batch,\n",
    "                            y: y_train_batch,\n",
    "                            phase_train: True,\n",
    "                            keep_prob: 0.5}\n",
    "        _, c = sess.run([train_op,cost], feed_dict=train_input_dict)\n",
    "    \n",
    "        if i % 5 == 0:\n",
    "            print(\"We are in epoch: \" +str(i))\n",
    "            print(\"Cost:\" +  str(c))\n",
    "            train_accuracy = accuracy.eval(feed_dict=train_input_dict)\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "            test_input_dict = {X_first: X1_test[0:50],\n",
    "                               X_second: X2_test[0:50],\n",
    "                               y: y_test[0:100],\n",
    "                               phase_train:False,\n",
    "                               keep_prob: 1.0}\n",
    "            predictions = sess.run(predict_op, feed_dict=test_input_dict)\n",
    "            print('Epoch : ', i,  'AUC : ', sm.roc_auc_score(y_test, predictions, average='samples'))\n",
    "            print('test accuracy %g' % accuracy.eval(feed_dict=test_input_dict))\n",
    "        #test_predictions = np.array([0 if t < 0.5 else 1 for t in test_predictions])\n",
    "        #print('Epoch : ', i,  'F1_score : ', sm.f1_score(y_test, test_predictions, pos_label=0))\n",
    "        # print(i, np.mean(np.argmax(y_test[test_indices], axis=1) == predictions))\n",
    "        # print sort_result(tags, predictions)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-191-30602697b5a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x239d13d77b8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [[1],[0],[0],[0],[1],[1]]\n",
    "B = [[0.2],[0.5],[0.6],[0.3],[0.2],[0.8]]\n",
    "np.argmax(B,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indicators: 0 One Hot[[1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X1,X2,indicators = form_verification_dataset(melspectrogram,one_hot_labels,sample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1102909, -0.1102909, -0.1102909, ..., -0.1102909, -0.1102909,\n",
       "        -0.1102909],\n",
       "       [-0.1102909, -0.1102909, -0.1102909, ..., -0.1102909, -0.1102909,\n",
       "        -0.1102909],\n",
       "       [-0.1102909, -0.1102909, -0.1102909, ..., -0.1102909, -0.1102909,\n",
       "        -0.1102909],\n",
       "       ...,\n",
       "       [-0.1102909, -0.1102909, -0.1102909, ..., -0.1102909, -0.1102909,\n",
       "        -0.1102909],\n",
       "       [-0.1102909, -0.1102909, -0.1102909, ..., -0.1102909, -0.1102909,\n",
       "        -0.1102909],\n",
       "       [-0.1102909, -0.1102909, -0.1102909, ..., -0.1102909, -0.1102909,\n",
       "        -0.1102909]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X1[0] - np.mean(X1[0]))/ np.std(X1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory requirements (in MB) if X1 was of type float16: 306\n",
      "Memory requirements (in MB) if X1 was of type float32: 612\n",
      "Memory requirements (in MB) if X1 was of type float64: 1225\n"
     ]
    }
   ],
   "source": [
    "#List of optimizations\n",
    "print(\"Memory requirements if vectors were of type float16: %d MB\"%(X1.astype(\"float16\").nbytes//(1024*1024)))\n",
    "print(\"Memory requirements if vectors were of type float32: %d MB\"%(X1.astype(\"float32\").nbytes//(1024*1024)))\n",
    "print(\"Memory requirements if vectors were of type float64: %d MB\"%(X1.astype(\"float64\").nbytes//(1024*1024)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
