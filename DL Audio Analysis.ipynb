{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa \n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from scipy import misc\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(music_dir):\n",
    "    \n",
    "    folders = [os.path.join(music_dir,folder) for folder in list(os.walk(music_dir))[0][1]]\n",
    "    print(folders[0])\n",
    "    filenames = [[os.path.join(folder,f) for f in list(os.walk(folder))[0][2]] for folder in folders]\n",
    "    filenames = [item for sublist in filenames for item in sublist]\n",
    "    files = [item for item in filenames]\n",
    "    track_ids = [str(int(filename.split('\\\\')[-1].split('.')[0])) for filename in filenames]\n",
    "    print(files[0:10])\n",
    "    print(track_ids[0:10])\n",
    "    print(\"Number of files: \" + str(len(files)))\n",
    "    return files, track_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOT NEEDED for current analysis.\n",
    "Load audio files, aggregate them over 100 sample intervals. Find mean, max and min for the features.\n",
    "\"\"\"\n",
    "def process_audio(files,track_ids,window = 100,sample = SAMPLE):\n",
    "    clip_range = int(19800*SAMPLE/window)  ##To clip the feature vectors for equal length.\n",
    "    train_data = []\n",
    "    num_files = len(files[0:sample])\n",
    "    for i in tqdm(range(num_files)):\n",
    "        audio_vec = []\n",
    "        audio, _ = librosa.load(files[i])\n",
    "        audio = np.reshape(audio,(-1))\n",
    "        for j in range(1,int(audio.shape[0]/window)):\n",
    "            mean_val = np.mean(audio[j*window: min((j+1)*window,audio.shape[0])])\n",
    "            max_val = np.max(audio[j*window: min((j+1)*window,audio.shape[0])])\n",
    "            min_val = np.min(audio[j*window: min((j+1)*window,audio.shape[0])])\n",
    "            #Append to audio vector for this audio file.\n",
    "            audio_vec += [mean_val,max_val,min_val]\n",
    "        audio_vec = np.array(audio_vec[:clip_range])\n",
    "        train_data.append(np.array(audio_vec))\n",
    "    train_data = np.array(train_data)\n",
    "    print(\"Original audio shape: \" + str(audio.shape))\n",
    "    print(\"Condensed audio shape: \" + str(train_data.shape))\n",
    "    train_ids = track_ids[0:num_files]\n",
    "    return train_data, train_ids\n",
    "train_data, train_ids = process_audio(files,track_ids, 200,200)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now aggregate results found in tracks.csv and get the labels.\n",
    "\n",
    "def load_tracks_file(filepath):\n",
    "    tracks = pd.read_csv(filepath)\n",
    "    ##Set new columns for the dataframe and remove the multi-index.\n",
    "    new_cols = tracks.iloc[0]\n",
    "    tracks = tracks.iloc[1:]\n",
    "    new_cols[0] = \"track_id\"\n",
    "    tracks.columns = new_cols\n",
    "    labels = tracks[\"genre_top\"]\n",
    "    \n",
    "    #Track id column should be string type\n",
    "    tracks.track_id = tracks.track_id.astype(int).astype(str)\n",
    "    #tracks = tracks.set_index(\"track_id\")\n",
    "    return tracks,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the melspectrogram from the audio files.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import librosa as lb\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "Fs         = 12000\n",
    "N_FFT      = 512\n",
    "N_MELS     = 96\n",
    "N_OVERLAP  = 256\n",
    "DURA       = 29.12\n",
    "\n",
    "def log_scale_melspectrogram(path, plot=False):\n",
    "    signal, sr = lb.load(path, sr=Fs)\n",
    "\n",
    "    n_sample = signal.shape[0]\n",
    "    n_sample_fit = int(DURA*Fs)\n",
    "    \n",
    "    if n_sample < n_sample_fit:\n",
    "        signal = np.hstack((signal, np.zeros((int(DURA*Fs) - n_sample,))))\n",
    "    elif n_sample > n_sample_fit:\n",
    "        signal = signal[round((n_sample-n_sample_fit)/2):round((n_sample+n_sample_fit)/2)]\n",
    "    \n",
    "    melspect = lb.amplitude_to_db(lb.feature.melspectrogram(y=signal, sr=Fs, hop_length=N_OVERLAP, n_fft=N_FFT, n_mels=N_MELS)**2, ref=1.0)\n",
    "\n",
    "    if plot:\n",
    "        melspect = melspect[np.newaxis, :]\n",
    "        plt.imshow(melspect.reshape((melspect.shape[1],melspect.shape[2])))\n",
    "        print(melspect.shape)\n",
    "\n",
    "    return melspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_mel_features(filenames):\n",
    "    arr = np.zeros([len(filenames), 96, 1366])\n",
    "    track_ids = [str(int(filename.split('\\\\')[-1].split('.')[0])) for filename in filenames]\n",
    "    for i in tqdm(range(len(filenames))):\n",
    "        mel = log_scale_melspectrogram(filenames[i], plot = False)\n",
    "        arr[i,:,:] = mel\n",
    "    return arr, track_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./music_samples/000\n",
      "['./music_samples/000\\\\000002.mp3', './music_samples/000\\\\000005.mp3', './music_samples/000\\\\000010.mp3', './music_samples/000\\\\000140.mp3', './music_samples/000\\\\000141.mp3', './music_samples/000\\\\000148.mp3', './music_samples/000\\\\000182.mp3', './music_samples/000\\\\000190.mp3', './music_samples/000\\\\000193.mp3', './music_samples/000\\\\000194.mp3']\n",
      "['2', '5', '10', '140', '141', '148', '182', '190', '193', '194']\n",
      "Number of files: 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [04:11<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "#Load audio\n",
    "music_dir = \"./music_samples/\"\n",
    "track_details_path = \"./tracks_small.csv\"\n",
    "files, track_ids = load_audio(music_dir)\n",
    "\n",
    "#Load csv file\n",
    "filepath = \"./tracks_small.csv\"\n",
    "tracks_df, labels = load_tracks_file(filepath)\n",
    "\n",
    "#Process melspectrograms from the audio files\n",
    "melspectrogram, track_ids = obtain_mel_features(files[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1)\n",
      "One hot label samples: \n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now, to determine the labels for the audio files that we have.\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def find_new_labels(train_ids, tracks_df):\n",
    "    labels_final = []\n",
    "    for tr in train_ids:\n",
    "        labels_final.append(tracks_df[tracks_df.track_id == tr][\"genre_top\"].values[0])\n",
    "    #One hot encode\n",
    "    label_dic = {label: i for (i,label) in enumerate(np.unique(labels_final))} #To store numerical values for each label\n",
    "    label_nums = np.array([label_dic[l] for l in labels_final]).reshape((-1,1))\n",
    "    print(label_nums.shape)\n",
    "    one_hot_labels = OneHotEncoder().fit_transform(label_nums).todense()\n",
    "    return one_hot_labels, label_dic\n",
    "\n",
    "def make_splits(melspectrogram,one_hot_labels,test_size = 0.33):\n",
    "    return train_test_split(melspectrogram, one_hot_labels, test_size = test_size, random_state = 42)\n",
    "\n",
    "\n",
    "one_hot_labels, label_dic = find_new_labels(track_ids,tracks_df)\n",
    "\n",
    "print(\"One hot label samples: \")\n",
    "print(one_hot_labels)\n",
    "\n",
    "\n",
    "    \n",
    "#X_train, X_test, y_train, y_test = make_splits(melspectrogram,one_hot_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({\"melspectrogram\": melspectrogram, \"one_hot_labels\":one_hot_labels},open(\"audio_data_labels.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load pickle file\n",
    "\"\"\"\n",
    "import pickle\n",
    "temp = pickle.load(open('audio_data_labels.pkl','rb'))\n",
    "melspectrogram = temp[\"melspectrogram\"]\n",
    "one_hot_labels = temp[\"one_hot_labels\"]\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(x, n_out, phase_train, scope='bn'):\n",
    "    with tf.variable_scope(scope):\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        mean, var = tf.cond(phase_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed\n",
    "\n",
    "def cnn(melspectrogram, weights, phase_train):\n",
    "    \n",
    "    n_mels, mel_vals = melspectrogram.get_shape()[1],melspectrogram.get_shape()[2]\n",
    "    \n",
    "    x = tf.reshape(melspectrogram,[-1,1,n_mels,mel_vals])\n",
    "    x = batch_norm(melspectrogram, mel_vals, phase_train)\n",
    "    x = tf.reshape(melspectrogram,[-1,n_mels,mel_vals,1])\n",
    "    conv2_1 = tf.add(tf.nn.conv2d(x, weights['wconv1'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv1'])\n",
    "    conv2_1 = tf.nn.relu(batch_norm(conv2_1, 32, phase_train))\n",
    "    mpool_1 = tf.nn.max_pool(conv2_1, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    dropout_1 = tf.nn.dropout(mpool_1, 0.5)\n",
    "\n",
    "    conv2_2 = tf.add(tf.nn.conv2d(dropout_1, weights['wconv2'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv2'])\n",
    "    conv2_2 = tf.nn.relu(batch_norm(conv2_2, 128, phase_train))\n",
    "    mpool_2 = tf.nn.max_pool(conv2_2, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    dropout_2 = tf.nn.dropout(mpool_2, 0.5)\n",
    "\n",
    "    conv2_3 = tf.add(tf.nn.conv2d(dropout_2, weights['wconv3'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv3'])\n",
    "    conv2_3 = tf.nn.relu(batch_norm(conv2_3, 128, phase_train))\n",
    "    mpool_3 = tf.nn.max_pool(conv2_3, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    dropout_3 = tf.nn.dropout(mpool_3, 0.5)\n",
    "\n",
    "    conv2_4 = tf.add(tf.nn.conv2d(dropout_3, weights['wconv4'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv4'])\n",
    "    conv2_4 = tf.nn.relu(batch_norm(conv2_4, 192, phase_train))\n",
    "    mpool_4 = tf.nn.max_pool(conv2_4, ksize=[1, 3, 5, 1], strides=[1, 3, 5, 1], padding='VALID')\n",
    "    dropout_4 = tf.nn.dropout(mpool_4, 0.5)\n",
    "\n",
    "    conv2_5 = tf.add(tf.nn.conv2d(dropout_4, weights['wconv5'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv5'])\n",
    "    conv2_5 = tf.nn.relu(batch_norm(conv2_5, 256, phase_train))\n",
    "    mpool_5 = tf.nn.max_pool(conv2_5, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='VALID')\n",
    "    dropout_5 = tf.nn.dropout(mpool_5, 0.5)\n",
    "\n",
    "    flat = tf.reshape(dropout_5, [-1, weights['woutput'].get_shape().as_list()[0]])\n",
    "    p_y_X = tf.nn.sigmoid(tf.add(tf.matmul(flat,weights['woutput']),weights['boutput']))\n",
    "    print(p_y_X.get_shape())\n",
    "\n",
    "    return p_y_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def init_biases(shape):\n",
    "    return tf.Variable(tf.zeros(shape))\n",
    "\n",
    "weights = {\n",
    "        'wconv1':init_weights([3, 3, 1, 32]),\n",
    "        'wconv2':init_weights([3, 3, 32, 128]),\n",
    "        'wconv3':init_weights([3, 3, 128, 128]),\n",
    "        'wconv4':init_weights([3, 3, 128, 192]),\n",
    "        'wconv5':init_weights([3, 3, 192, 256]),\n",
    "        'bconv1':init_biases([32]),\n",
    "        'bconv2':init_biases([128]),\n",
    "        'bconv3':init_biases([128]),\n",
    "        'bconv4':init_biases([192]),\n",
    "        'bconv5':init_biases([256]),\n",
    "        'woutput':init_weights([256, len(label_dic.keys())]),\n",
    "        'boutput':init_biases([len(label_dic.keys())])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Hip-Hop', 'Electronic', 'Folk', 'International', 'Pop', 'Experimental', 'Rock'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got -1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-0371e724fd8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mphase_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'phase_train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0my_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-1e9582b42227>\u001b[0m in \u001b[0;36mcnn\u001b[1;34m(melspectrogram, weights, phase_train)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mn_mels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmel_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmelspectrogram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmel_vals\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmel_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmel_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   2446\u001b[0m   \"\"\"\n\u001b[0;32m   2447\u001b[0m   result = _op_def_lib.apply_op(\"Reshape\", tensor=tensor, shape=shape,\n\u001b[1;32m-> 2448\u001b[1;33m                                 name=name)\n\u001b[0m\u001b[0;32m   2449\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    491\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m               raise TypeError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    488\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    491\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    174\u001b[0m                                          as_ref=False):\n\u001b[0;32m    175\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m    163\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m--> 165\u001b[1;33m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    166\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    439\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnumpy_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[0mproto_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FlattenToStrings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    439\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnumpy_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[0mproto_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FlattenToStrings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[1;32m---> 65\u001b[1;33m                     (bytes_or_text,))\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected binary or unicode string, got -1"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as sm\n",
    "batch_size    = 1\n",
    "learning_rate = 0.003\n",
    "n_epoch       = 50\n",
    "n_samples     = len(X_train)                              # change to 1000 for entire dataset\n",
    "cv_split      = 0.8                             \n",
    "train_size    = int(n_samples * cv_split)                               \n",
    "test_size     = n_samples - train_size\n",
    "\n",
    "\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 96, melspectrogram.shape[2], 1])\n",
    "y = tf.placeholder(\"float\", [None, len(label_dic.keys())])\n",
    "lrate = tf.placeholder(\"float\")\n",
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "y_ = cnn(X, weights, phase_train)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = y_))\n",
    "train_op = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost)\n",
    "predict_op = y_\n",
    "\n",
    "tags = sorted(label_dic.keys())\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "    return np.asarray(data_shuffle).reshape((batch_size,96,1366,1)), np.asarray(labels_shuffle)#.reshape((batch_size,96,1366,1)) \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for i in range(n_epoch):\n",
    "        print(i)\n",
    "        #training_batch = zip(range(0, train_size, batch_size),range(batch_size, train_size+1, batch_size))\n",
    "        for j in range(50):\n",
    "            X_train_batch,y_train_batch = next_batch(batch_size,X_train,y_train)\n",
    "            train_input_dict = {X: X_train_batch, \n",
    "                                y: y_train_batch,\n",
    "                                lrate: learning_rate,\n",
    "                                phase_train: True}\n",
    "            sess.run(train_op, feed_dict=train_input_dict)\n",
    "        #test_indices = np.arange(len(X_test))\n",
    "        #np.random.shuffle(test_indices)\n",
    "        #test_indices = test_indices[0:test_size]\n",
    "\n",
    "        test_input_dict = {X: X_test.reshape(-1,96,1366,1),\n",
    "                           y: y_test,\n",
    "                           phase_train:True}\n",
    "        predictions = sess.run(predict_op, feed_dict=test_input_dict)\n",
    "        print('Epoch : ', i,  'AUC : ', sm.roc_auc_score(y_test, predictions, average='samples'))\n",
    "        # print(i, np.mean(np.argmax(y_test[test_indices], axis=1) == predictions))\n",
    "        # print sort_result(tags, predictions)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converts original dataset into the verification format. i.e. Input spectrograms X1, X2 and an output of 0 if they're of \n",
    "dissimilar genre and 1 if they're of the same genre.\n",
    "Input: data , labels, sample (How many input samples do you want to transform?)\n",
    "Output: Three lists X1, X2, indicators\n",
    "\"\"\"\n",
    "def form_verification_dataset(data,labels,sample=10):\n",
    "    ##Insert assert statements here for the correct input data sizes\n",
    "    assert len(data) == len(labels)\n",
    "    #Select samples\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:sample]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "    \n",
    "    #Pair each sample with every other sample from the dataset. O(sample^2)\n",
    "    num_mels,mel_vals = data.shape[1], data.shape[2]\n",
    "    X1 = X2 = np.zeros((sample*(sample-1)//2,num_mels,mel_vals))\n",
    "    indicators = []\n",
    "    count = 0\n",
    "    for i in range(0,len(data_shuffle)):\n",
    "        for j in range(i+1,len(data_shuffle)):\n",
    "            X1[count,:,:] = data_shuffle[i]\n",
    "            X1[count,:,:] = data_shuffle[j]\n",
    "            if np.equal(labels_shuffle[i], labels_shuffle[j]).all():\n",
    "                indicators.append(0)\n",
    "            else:\n",
    "                indicators.append(1)\n",
    "            count += 1\n",
    "            \n",
    "\n",
    "    return X1, X2, np.array(indicators)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def init_biases(shape):\n",
    "    return tf.Variable(tf.zeros(shape))\n",
    "\n",
    "weights = {\n",
    "        'wconv1':init_weights([3, 3, 1, 32]),\n",
    "        'wconv2':init_weights([3, 3, 32, 128]),\n",
    "        'wconv3':init_weights([3, 3, 128, 128]),\n",
    "        'wconv4':init_weights([3, 3, 128, 192]),\n",
    "        'wconv5':init_weights([3, 3, 192, 256]),\n",
    "        'bconv1':init_biases([32]),\n",
    "        'bconv2':init_biases([128]),\n",
    "        'bconv3':init_biases([128]),\n",
    "        'bconv4':init_biases([192]),\n",
    "        'bconv5':init_biases([256]),\n",
    "        'woutput':init_weights([256, 128]),\n",
    "        'boutput':init_biases([128]),\n",
    "        'woutput2':init_weights([256, 128]),\n",
    "        'boutput2':init_biases([128]),\n",
    "        'wfinal':init_weights([128, 1]),\n",
    "        'bfinal':init_biases([1]),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0.2\n",
    "\n",
    "def dual_cnn(X_first,X_second, weights, phase_train):\n",
    "    \n",
    "    #assert X_first.shape == X_second.shape, str(X_first.shape) + \" does not match with \" + str(X_second.shape)\n",
    "    #assert indicators.shape[0] == X_first.shape[0], \"Indicator array is not of length \" + indicators.shape[0]\n",
    "    \n",
    "    num_samples, n_mels, mel_vals = X1.shape[0],X1.shape[1],X1.shape[2]\n",
    "    \n",
    "    x_first = tf.reshape(X_first,[-1,1,n_mels,mel_vals])\n",
    "    x_second = tf.reshape(X_second,[-1,1,n_mels,mel_vals])\n",
    "    \n",
    "    x_first = batch_norm(x_first, mel_vals, phase_train)\n",
    "    x_second = batch_norm(x_first, mel_vals, phase_train)\n",
    "    \n",
    "    x_first = tf.reshape(x_first,[-1,n_mels,mel_vals,1])\n",
    "    x_second = tf.reshape(x_second,[-1,n_mels,mel_vals,1])\n",
    "    \n",
    "    conv2_1_first = tf.add(tf.nn.conv2d(x_first, weights['wconv1'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv1'])\n",
    "    conv2_1_second = tf.add(tf.nn.conv2d(x_second, weights['wconv1'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv1'])\n",
    "    \n",
    "    conv2_1_first = tf.nn.relu(batch_norm(conv2_1_first, 32, phase_train))\n",
    "    conv2_1_second = tf.nn.relu(batch_norm(conv2_1_second, 32, phase_train))\n",
    "    \n",
    "    mpool_1_first = tf.nn.max_pool(conv2_1_first, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    mpool_1_second = tf.nn.max_pool(conv2_1_second, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    \n",
    "    dropout_1_first = tf.nn.dropout(mpool_1_first, 0.5)\n",
    "    dropout_1_second = tf.nn.dropout(mpool_1_second, 0.5)\n",
    "\n",
    "    conv2_2_first = tf.add(tf.nn.conv2d(dropout_1_first, weights['wconv2'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv2'])\n",
    "    conv2_2_second = tf.add(tf.nn.conv2d(dropout_1_second, weights['wconv2'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv2'])\n",
    "    \n",
    "    conv2_2_first = tf.nn.relu(batch_norm(conv2_2_first, 128, phase_train))\n",
    "    conv2_2_second = tf.nn.relu(batch_norm(conv2_2_first, 128, phase_train))\n",
    "    \n",
    "    mpool_2_first = tf.nn.max_pool(conv2_2_first, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    mpool_2_second = tf.nn.max_pool(conv2_2_first, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    \n",
    "    \n",
    "    dropout_2_first = tf.nn.dropout(mpool_2_first, 0.5)\n",
    "    dropout_2_second = tf.nn.dropout(mpool_2_second, 0.5)\n",
    "\n",
    "    conv2_3_first = tf.add(tf.nn.conv2d(dropout_2_first, weights['wconv3'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv3'])\n",
    "    conv2_3_second = tf.add(tf.nn.conv2d(dropout_2_second, weights['wconv3'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv3'])\n",
    "    \n",
    "\n",
    "    conv2_3_first = tf.nn.relu(batch_norm(conv2_3_first, 128, phase_train))\n",
    "    conv2_3_second = tf.nn.relu(batch_norm(conv2_3_second, 128, phase_train))\n",
    "    \n",
    "    mpool_3_first = tf.nn.max_pool(conv2_3_first, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    mpool_3_second = tf.nn.max_pool(conv2_3_second, ksize=[1, 2, 4, 1], strides=[1, 2, 4, 1], padding='VALID')\n",
    "    \n",
    "    dropout_3_first = tf.nn.dropout(mpool_3_first, 0.5)\n",
    "    dropout_3_second = tf.nn.dropout(mpool_3_second, 0.5)\n",
    "\n",
    "    conv2_4_first = tf.add(tf.nn.conv2d(dropout_3_first, weights['wconv4'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv4'])\n",
    "    conv2_4_second = tf.add(tf.nn.conv2d(dropout_3_second, weights['wconv4'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv4'])\n",
    "    \n",
    "    conv2_4_first = tf.nn.relu(batch_norm(conv2_4_first, 192, phase_train))\n",
    "    conv2_4_second = tf.nn.relu(batch_norm(conv2_4_second, 192, phase_train))\n",
    "    \n",
    "    mpool_4_first = tf.nn.max_pool(conv2_4_first, ksize=[1, 3, 5, 1], strides=[1, 3, 5, 1], padding='VALID')\n",
    "    mpool_4_second = tf.nn.max_pool(conv2_4_second, ksize=[1, 3, 5, 1], strides=[1, 3, 5, 1], padding='VALID')\n",
    "    \n",
    "    dropout_4_first = tf.nn.dropout(mpool_4_first, 0.5)\n",
    "    dropout_4_second = tf.nn.dropout(mpool_4_second, 0.5)\n",
    "\n",
    "    conv2_5_first = tf.add(tf.nn.conv2d(dropout_4_first, weights['wconv5'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv5'])\n",
    "    conv2_5_second = tf.add(tf.nn.conv2d(dropout_4_second, weights['wconv5'], strides=[1, 1, 1, 1], padding='SAME'), weights['bconv5'])\n",
    "    \n",
    "    \n",
    "    conv2_5_first = tf.nn.relu(batch_norm(conv2_5_first, 256, phase_train))\n",
    "    conv2_5_second = tf.nn.relu(batch_norm(conv2_5_second, 256, phase_train))\n",
    "    \n",
    "    mpool_5_first = tf.nn.max_pool(conv2_5_first, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='VALID')\n",
    "    mpool_5_second = tf.nn.max_pool(conv2_5_second, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='VALID')\n",
    "    \n",
    "    dropout_5_first = tf.nn.dropout(mpool_5_first, 0.5)\n",
    "    dropout_5_second = tf.nn.dropout(mpool_5_second, 0.5)\n",
    "    \n",
    "    flat_first = tf.reshape(dropout_5_first, [-1, weights['woutput'].get_shape().as_list()[0]])\n",
    "    flat_second = tf.reshape(dropout_5_second, [-1, weights['woutput'].get_shape().as_list()[0]])\n",
    "    \n",
    "    print(flat_first.get_shape())\n",
    "    print(flat_second.get_shape())\n",
    "    flat_first = tf.add(tf.matmul(flat_first, weights['woutput2']), weights['boutput2'])\n",
    "    flat_second = tf.add(tf.matmul(flat_second, weights['woutput2']), weights['boutput2'])\n",
    "    \n",
    "    flat_first = tf.nn.relu(flat_first)\n",
    "    flat_second = tf.nn.relu(flat_second)\n",
    "    # Apply Dropout\n",
    "    flat_first = tf.nn.dropout(flat_first, 0.4)\n",
    "    flat_second = tf.nn.dropout(flat_second, 0.4)\n",
    "    \n",
    "    print(flat_first.get_shape())\n",
    "    print(flat_second.get_shape())\n",
    "    \n",
    "    final_layer = tf.reshape(tf.concat(0, [flat_first, flat_second]),(-1,1))\n",
    "    #final_layer = tf.add(tf.matmul(final_layer, weights['wfinal']), weights['bfinal'])\n",
    "    print(final_layer.get_shape())\n",
    "    \"\"\"\n",
    "    ##Add fully connected layers here\n",
    "    dense_first = tf.layers.dense(inputs=flat_first, units=128, activation=tf.nn.relu)\n",
    "    dense_second = tf.layers.dense(inputs=flat_second, units=128, activation=tf.nn.relu)\n",
    "    print(dense_first.shape)\n",
    "    print(dense_second.shape)\n",
    "    final_layer = tf.reshape(tf.concat([dense_first, dense_second], 0),(1,-1))\n",
    "    print(final_layer.shape)\n",
    "    \"\"\"\n",
    "    p_y_X = tf.nn.sigmoid(tf.add(tf.matmul(final_layer,weights['wfinal']),weights['bfinal']))\n",
    "    print(p_y_X.get_shape())\n",
    "    return p_y_X\n",
    "    \"\"\"\n",
    "    #Contrastive loss\n",
    "    d = tf.reduce_sum(tf.square(dense_first - dense_second), 1)\n",
    "    d_sqrt = tf.sqrt(d)\n",
    "    loss = label * tf.square(tf.maximum(0., margin - d_sqrt)) + (1 - label) * d\n",
    "    loss = 0.5 * tf.reduce_mean(loss)\n",
    "    return loss\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 42, 15, 9, 16, 37, 31, 27, 0, 30, 29, 5, 11, 33, 1, 40, 21, 2, 34, 23, 36, 10, 22, 18, 44, 20, 7, 14, 28, 38] [39, 25, 26, 43, 35, 41, 4, 12, 8, 3, 6, 24, 32, 19, 17]\n",
      "(?, 256)\n",
      "(?, 256)\n",
      "(?, 128)\n",
      "(?, 128)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected int32, got list containing Tensors of type '_Message' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-1b00724c6454>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mphase_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'phase_train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0my_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdual_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_first\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_second\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-405a79623346>\u001b[0m in \u001b[0;36mdual_cnn\u001b[1;34m(X_first, X_second, weights, phase_train)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_second\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mfinal_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mflat_first\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_second\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;31m#final_layer = tf.add(tf.matmul(final_layer, weights['wfinal']), weights['bfinal'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   1170\u001b[0m       ops.convert_to_tensor(\n\u001b[0;32m   1171\u001b[0m           \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"concat_dim\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1172\u001b[1;33m           \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1173\u001b[0m               tensor_shape.scalar())\n\u001b[0;32m   1174\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    233\u001b[0m                                          as_ref=False):\n\u001b[0;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m    212\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m--> 214\u001b[1;33m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    431\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m       \u001b[0m_AssertCompatible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m       \u001b[1;31m# check to them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[1;34m(values, dtype)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[1;32m--> 344\u001b[1;33m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected int32, got list containing Tensors of type '_Message' instead."
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as sm\n",
    "batch_size    = 1\n",
    "learning_rate = 0.003\n",
    "n_epoch       = 50\n",
    "n_samples     = len(melspectrogram)                              # change to 1000 for entire dataset\n",
    "cv_split      = 0.8                             \n",
    "train_size    = int(n_samples * cv_split)                               \n",
    "test_size     = n_samples - train_size\n",
    "\n",
    "#Load the dataset\n",
    "X1,X2,indicators = form_verification_dataset(melspectrogram,one_hot_labels,sample=10)\n",
    "num_samples, n_mels, mel_vals = X1.shape[0],X1.shape[1],X1.shape[2]\n",
    "\n",
    "#Split into training and testing\n",
    "def make_splits_verif(X1,X2,indicators):\n",
    "    inds = range(0,len(X1))\n",
    "    train_inds,test_inds,_,_ = train_test_split(inds,inds,random_state = 42, test_size = 0.33)\n",
    "    print(train_inds,test_inds)\n",
    "    X1_train = [X1[idx] for idx in train_inds]\n",
    "    X1_test = [X1[idx] for idx in test_inds]\n",
    "    \n",
    "    X2_train = [X2[idx] for idx in train_inds]\n",
    "    X2_test = [X1[idx] for idx in test_inds]\n",
    "    \n",
    "    X1_test = np.asarray(X1_test).reshape(-1, n_mels,mel_vals,1)\n",
    "    X2_test = np.asarray(X2_test).reshape(-1, n_mels,mel_vals,1)\n",
    "    \n",
    "    y_train, y_test = np.reshape([indicators[idx] for idx in train_inds],(-1,1)), np.reshape([indicators[idx] for idx in test_inds],(-1,1))\n",
    "    return X1_train, X2_train, y_train, X1_test, X2_test, y_test\n",
    "\n",
    "def next_batch_verif(num, X1, X2, indicators):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    num_samples, n_mels, mel_vals = X1.shape[0],X1.shape[1],X1.shape[2]\n",
    "    idx = np.arange(0 , len(X1))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    X1_shuffle = [X1[i] for i in idx]\n",
    "    X2_shuffle = [X2[i] for i in idx]\n",
    "    indicators_shuffle = [indicators[i] for i in idx]\n",
    "    return (np.asarray(X1_shuffle).reshape((num,n_mels,mel_vals,1)), \n",
    "            np.asarray(X1_shuffle).reshape((num,n_mels,mel_vals,1)), \n",
    "            np.asarray(indicators_shuffle).reshape(-1,1))\n",
    "\n",
    "\n",
    "#Make data splits\n",
    "X1_train, X2_train, y_train, X1_test, X2_test, y_test = make_splits_verif(X1,X2,indicators)\n",
    "\n",
    "X_first = tf.placeholder(\"float\", [None, X1.shape[1], X1.shape[2], 1],name=\"First_input_vector\")\n",
    "X_second = tf.placeholder(\"float\", [None, X1.shape[1], X1.shape[2], 1],name=\"Second_input_vector\")\n",
    "\n",
    "y = tf.placeholder(\"float\", [None, 1],name=\"Truth_labels\")\n",
    "lrate = tf.placeholder(\"float\",name=\"Learning_rate\")\n",
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "y_ = dual_cnn(X_first, X_second, weights, phase_train)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = y_))\n",
    "train_op = tf.train.RMSPropOptimizer(lrate, 0.9).minimize(cost)\n",
    "predict_op = y_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for i in range(n_epoch):\n",
    "        print(i)\n",
    "        #training_batch = zip(range(0, train_size, batch_size),range(batch_size, train_size+1, batch_size))\n",
    "        for j in range(50):\n",
    "            X1_train_batch,X2_train_batch,y_train_batch = next_batch_verif(batch_size,X1,X2,indicators)\n",
    "            print(X1_train_batch.shape,X2_train_batch.shape,y_train_batch.shape )\n",
    "            train_input_dict = {X_first: X1_train_batch,\n",
    "                                X_second: X2_train_batch,\n",
    "                                y: y_train_batch,\n",
    "                                lrate: learning_rate,\n",
    "                                phase_train: [True]}\n",
    "            sess.run(train_op, feed_dict=train_input_dict)\n",
    "        \n",
    "        \n",
    "        test_input_dict = {X_first: X1_test,\n",
    "                           X_second: X2_test,\n",
    "                           y: y_test,\n",
    "                           phase_train:[False]}\n",
    "        predictions = sess.run(predict_op, feed_dict=test_input_dict)\n",
    "        print('Epoch : ', i,  'AUC : ', sm.roc_auc_score(y_test, predictions, average='samples'))\n",
    "        # print(i, np.mean(np.argmax(y_test[test_indices], axis=1) == predictions))\n",
    "        # print sort_result(tags, predictions)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.bool"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-2f222e14af66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindicators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mform_verification_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-161-2685c66224b1>\u001b[0m in \u001b[0;36mform_verification_dataset\u001b[1;34m(data, labels, sample)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mform_verification_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m##Insert assert statements here for the correct input data sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;31m#Select samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X1,X2,indicators = form_verification_dataset(X_train,y_train,sample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-191-30602697b5a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x19ffc8027f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
